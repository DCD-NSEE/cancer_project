{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cancer Libraries and functions.ipynb","provenance":[],"collapsed_sections":["LPae7x8J38aH","otqVd1m7YMs1","ImgVOwGnmMB8","tUQsVex8mQZp","Z85PSEp0ROLU","_kpzdjNIKXZz","ZheaeDLZUWJh","73U-oFSDZrSl","LEvrA6yIpaX7","gFGUYhdDLQRk","6kMEltD5kqXx","szGQwL8iVN8V","BYXCXeSclD9W","6syIujtClMep","QYlf_a6BlhMX","NPUQy2jgdn15","EXs-tiaiVO1l","TdRaJ910s1uJ","WP4zex4i3cw8","E7HU7c1yDHVP","AFVw4Xenh_as","8WVIfoHVUJD5","Dhhu_98sWwEQ","RueRGXrmXp6z","dYcJTcD-Xs7x","UNVmfqcPXxAi","rUYZlEolM5b3","ufK57I-jIvL5","ombZvBB4Z0XT","dK3aK7YPUrJ7","Sgs0DgsWVfmD","QXtLVtVqaJ4R","7J57oHSLKQes","6XtOaE7uW66Z","buuhH7YtW-1g","8WfiE0yp5Rm8","qUbaARtuNgMu","FZQlzhLsX8TT","lNdcbiOxgUet","Wkxqo_7c7qNA","PFKZ0EQ0PJHt","h3crI-qE7qNU","FlRLT85q7qNj","3nMd35c_7qNm","xZxxgl1c7qNq","nRdZQgbP7qNv","XPNY0nc_7qNw","5sxMeojo7qNy","LUVJ5SWF7qN1","wzAYYDTR7qN2","uDikTsuh7qN3","SEthmGFq7qN6","wktYFbDgKneA","ijli2WBMPSOb","grcJxzdUKnea","knknMJ7_Kneg","5-C1GBrzKnej","FV5OiHr-Kne2","KkfvRh19Kne6","v34HAZ5jKne6","2U6IXnjRKne7","iRRgxiMOKne8","dWJgIEaFKne9","J2UkKhYAKne-","XvhRkiEWKnfA","-MyMxkqb3FHM","LcTMK7LxPWTk","BGwlRHNm3FHb","HVTXLDhh3FHe","jdBFN46J3FHf","BNNq_N0I3FHg","gzVB5dcj3FHh","8rh4zxWt3FHj","X_oRr3f6UeNG","sFkqTrU7PZ5e","HcBFvfaBYCEW","fyRUI_16YCEZ","58mMnWb3YCEe","bcybSi3MYCEe","hU77rkScaqPC","R8hPkMl0g1f8","t6Qn_RWr9wb4","q56VPI_0Pdrj","lSsL75pE9wcH","8XPyaSWF9wcN","1LU-mXgM9wcP","NpaxyVD79wcQ","28GrPJoS9wcS","qhCRRpew9wcT","pcL2DGLg8DF-"],"mount_file_id":"1fR9g2luc55coRAcv8aFtP6GrADp0y0XW","authorship_tag":"ABX9TyNoZR8uJGW67b2LeEipuQaQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LPae7x8J38aH"},"source":["# **Bibliotecas e instalações**"]},{"cell_type":"code","metadata":{"id":"f_Lpm8RjeXYW"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","import pickle\n","seed = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ivafM-qaTLo"},"source":["# Pré-Processamento\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, PowerTransformer, QuantileTransformer\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6QXRUiSfi7w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620389982115,"user_tz":180,"elapsed":3247,"user":{"displayName":"Lucas Buk Cardoso","photoUrl":"","userId":"13718893696373545505"}},"outputId":"2382a1b4-c546-4fac-e20f-07c73ef1616a"},"source":["# Classificação\n","from sklearn.decomposition import PCA\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","from sklearn.ensemble import RandomForestClassifier\n","import xgboost as xgb\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","from sklearn.metrics import plot_confusion_matrix"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning:\n","\n","The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:\n","\n","The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oMEBkOAi3U1I"},"source":["# Regressão\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otqVd1m7YMs1"},"source":["# **Funções**"]},{"cell_type":"markdown","metadata":{"id":"ImgVOwGnmMB8"},"source":["## **Gerais**"]},{"cell_type":"code","metadata":{"id":"q-lnAaaNYPbJ"},"source":["def read_csv(path):\n","  \"\"\"Read csv files\n","\n","  :param path str: path to the csv file.\n","\n","  :return: dataframe from the csv file.\n","  :rtype: pd.DataFrame\n","  \"\"\"\n","\n","  df = pd.read_csv(path)\n","  print(df.shape)\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_ogS1lWYkAw"},"source":["def save_csv(df, path):\n","  \"\"\"Save csv files\n","\n","  :param df pd.DataFrame: dataframe to be saved.\n","  :param path str: path to save the csv file.\n","\n","  :return: no value\n","  :rtype: none\n","  \"\"\"\n","\n","  df.to_csv(path, encoding='utf-8', index=False)\n","  print('Arquivo csv salvo com sucesso!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ghMwgbSVB-T"},"source":["def get_dates_diff(df):\n","  \"\"\"Get the difference, in days, between columns with dates\n","\n","  :param df pd.DataFrame: DataFrame to get the dates difference.\n","\n","  :return: DataFrame with dates difference in nine new columns \n","  :rtype: pd.DataFrame\n","  \"\"\"\n","  \n","  df_aux = df.copy()\n","  \n","  df_aux.dropna(subset=['DTTRAT','DTULTINFO'], inplace=True)\n","\n","  lista_datas = ['DTCONSULT', 'DTDIAG', 'DTTRAT', 'DTULTINFO', 'DTRECIDIVA']\n","  \n","  for c in lista_datas:\n","    if c == 'DTTRAT':\n","      fmt = '%d/%m/%Y'\n","    else:\n","      fmt = '%Y-%m-%d'\n","    df_aux[c] = pd.to_datetime(df_aux[c], format=fmt)\n","\n","  df_aux['delta_t1'] = (df_aux.DTDIAG - df_aux.DTCONSULT).dt.days\n","  df_aux['delta_t2'] = (df_aux.DTTRAT - df_aux.DTDIAG).dt.days\n","  df_aux['delta_t3'] = (df_aux.DTTRAT - df_aux.DTCONSULT).dt.days\n","\n","  df_aux['delta_t4'] = (df_aux.DTRECIDIVA - df_aux.DTCONSULT).dt.days\n","  df_aux['delta_t5'] = (df_aux.DTRECIDIVA - df_aux.DTDIAG).dt.days\n","  df_aux['delta_t6'] = (df_aux.DTRECIDIVA - df_aux.DTTRAT).dt.days\n","\n","  df_aux['delta_t7'] = (df_aux.DTULTINFO - df_aux.DTCONSULT).dt.days\n","  df_aux['delta_t8'] = (df_aux.DTULTINFO - df_aux.DTDIAG).dt.days\n","  df_aux['delta_t9'] = (df_aux.DTULTINFO - df_aux.DTTRAT).dt.days\n","\n","  return df_aux"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCzT84kOwx0D"},"source":["def get_labels(df):\n","  \"\"\"Create death labels acording to the last information year.\n","\n","  :param df pd.DataFrame: dataframe to be processed.\n","\n","  :return: DataFrame with the new labels\n","  :rtype: pd.DataFrame\n","  \"\"\"\n","\n","  df_aux = df.copy()\n","\n","  # Label de óbito\n","  df_aux['ob'] = 0\n","\n","  # Label de óbito de acordo com o ano\n","  df_aux['ano_ob'] = 0\n","  \n","  for index, row in df_aux.iterrows():\n","    if row.ULTINFO > 2:\n","      df_aux.loc[index, 'ob'] = 1\n","      if row.delta_t8 < 365:\n","        df_aux.loc[index, 'ano_ob'] = 1\n","      elif row.delta_t8 < 2*365:\n","        df_aux.loc[index, 'ano_ob'] = 2\n","      elif row.delta_t8 < 3*365:\n","        df_aux.loc[index, 'ano_ob'] = 3\n","      elif row.delta_t8 < 4*365:\n","        df_aux.loc[index, 'ano_ob'] = 4\n","      elif row.delta_t8 < 5*365:\n","        df_aux.loc[index, 'ano_ob'] = 5\n","      else:\n","        df_aux.loc[index, 'ano_ob'] = 6\n","\n","  return df_aux"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJrOmtBTUJEF"},"source":["def get_label_rec(df):\n","  \"\"\"Create the labels analyzing whether there was recurrence.\n","  \n","  :param df pd.DataFrame: dataframe to be processed.\n","\n","  :return: DataFrame with the new labels\n","  :rtype: pd.DataFrame\n","  \"\"\"\n","\n","  df_aux = df.copy()\n","\n","  df_aux['ob_com_rec'] = 0\n","  df_aux['ob_sem_rec'] = 0\n","  df_aux['vivo_com_rec'] = 0\n","  df_aux['vivo_sem_rec'] = 0\n","\n","  for i, row in df.iterrows():\n","    if row['ob'] == 1:\n","      if row.RECNENHUM == 1:\n","        df_aux.loc[i, 'ob_sem_rec'] = 1\n","      else:\n","        df_aux.loc[i, 'ob_com_rec'] = 1\n","        \n","    else:\n","      if row.RECNENHUM == 1:\n","        df_aux.loc[i, 'vivo_sem_rec'] = 1\n","      else:\n","        df_aux.loc[i, 'vivo_com_rec'] = 1\n","\n","  return df_aux"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tUQsVex8mQZp"},"source":["## **Modelos**"]},{"cell_type":"code","metadata":{"id":"LdQRlaLJ3sum"},"source":["def variables_preprocessing(df):\n","  \"\"\"Do some preprocessing on the DataFrame like strings splits, fill NaN values,\n","     replace values and drop some columns.\n","\n","  :param df pd.DataFrame: DataFrame to be preprocessed.\n","\n","  :return: DataFrame after be preprocessed and get some columns removed\n","  :rtype: pd.DataFrame\n","  \"\"\"\n","\n","  df_aux = df.copy()\n","  no_info = '**Sem informação**'\n","\n","  # DRS\n","  DRS_expand = df_aux.DRS.str.split(' ', expand=True)\n","  df_aux['DRS'] = DRS_expand[1]\n","  df_aux.DRS = df_aux.DRS.fillna(0).astype('int64')\n","\n","  # META\n","  df_aux.META01.fillna(no_info, inplace=True)\n","  df_aux.META02.fillna(no_info, inplace=True)\n","  df_aux.META03.fillna(no_info, inplace=True)\n","  df_aux.META04.fillna(no_info, inplace=True)\n","\n","  # REC\n","  df_aux.REC01.fillna(no_info, inplace=True)\n","  df_aux.REC02.fillna(no_info, inplace=True)\n","  df_aux.REC03.fillna(no_info, inplace=True)\n","  df_aux.REC04.fillna(no_info, inplace=True)\n","\n","  # PT\n","  df_aux.PT = df_aux.PT.replace([1.0],'1')\n","  df_aux.PT = df_aux.PT.str.upper()\n","  df_aux.PT.fillna(no_info, inplace=True)\n","\n","  # PN\n","  df_aux.PN = df_aux.PN.replace([0.0],'0')\n","  df_aux.PN = df_aux.PN.str.upper()\n","  df_aux.PN = df_aux.PN.replace(['1BI','IBII','O'],['1B1','1B2','0'])\n","  df_aux.PN.fillna(no_info, inplace=True)\n","\n","  # PM\n","  df_aux.PM = df_aux.PM.replace([0.0],'0')\n","  df_aux.PM.fillna(no_info, inplace=True)\n","\n","  # CICI\n","  df_aux.CICI.fillna(no_info, inplace=True)\n","\n","  # CICIGRUP\n","  CICIGRUP_expand = df_aux.CICIGRUP.str.split('  ', expand=True)\n","  df_aux['CICIGRUP'] = CICIGRUP_expand[0]\n","  df_aux.CICIGRUP.fillna(no_info, inplace=True)\n","\n","  # Colunas com valores únicos \n","  col = df_aux.columns\n","  drop_cols = ['S','QUIMIOANT','HORMOANT','TMOANT','IMUNOANT','OUTROANT','ERRO',\n","               'CIDO', 'UFNASC','CIDADE','DESCTOPO','DESCMORFO','DSCCIDO','CICISUBGRU',\n","               'INSTORIG', 'OUTRACLA']\n","\n","  col = col.drop(drop_cols)\n","\n","  return df_aux[col]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r69BRdrcISkx"},"source":["def get_train_test(df, drop_cols, label, test_size=0.25, random_state=10):\n","  \"\"\"Get features and label, and then returns train and test dataframes.\n","\n","  :param df pd.DataFrame: dataframe that will be splitted.\n","  :param drop_cols list: columns to be removed from the DataFrame.\n","  :param label str: name of the label column.\n","  :param test_size float: size of test (default=0.25).\n","  :param random_state int: value for train_test_split random_state (default=10).\n","\n","  :return: train and test DataFrames, X_train, X_test, y_train, y_test\n","  :rtype: pd.DataFrame\n","  \"\"\"\n","\n","  df_aux = df.copy()\n","\n","  cols = df_aux.columns.drop(drop_cols)\n","  lb = df_aux[label].copy()\n","  cols = cols.drop(label)\n","  feat = df_aux[cols]\n","\n","  X_train, X_test, y_train, y_test = train_test_split(feat, lb, test_size=test_size, random_state=random_state)\n","  print(f'X_train = {X_train.shape}, X_test = {X_test.shape}')\n","  print(f'y_train = {y_train.shape}, y_test = {y_test.shape}')\n","\n","  return X_train, X_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFAnM9raPU_T"},"source":["def train_preprocessing(df, normalizer='StandardScaler', pca=False, pca_components=None, random_state=10):\n","  \"\"\"Preprocessing the train dataset.\n","\n","  :param df pd.DataFrame: DataFrame to be preprocessed.\n","  :param normalizer str: which normalizer to be fitted to the data (default='StandardScaler').\n","  :param pca bool: if want to use PCA components set True (default=False).\n","  :param pca_components int: number of PCA components (default=None).\n","  :param random_state int: value for pca random_state (default=10).\n","\n","  :return df: preprocessed train DataFrame \n","  :rtype: pd.DataFrame\n","  :return enc: trained LabelEncoder \n","  :rtype: dict\n","  :return norm: trained normalizer \n","  :rtype: object\n","  :return pca if param pca=True: trained PCA \n","  :rtype: object\n","  \"\"\"\n","\n","  df_aux = df.copy()\n","\n","  list_categorical = df_aux.select_dtypes(include='object').columns\n","\n","  enc = dict()\n","  for col in list_categorical:\n","    enc[col] = LabelEncoder()\n","    df_aux[col] = enc[col].fit_transform(df_aux[col])\n","\n","  if normalizer == 'StandardScaler':\n","    norm = StandardScaler()\n","  elif normalizer == 'MinMaxScaler':\n","    norm = MinMaxScaler()\n","  elif normalizer == 'MaxAbsScaler':\n","    norm = MaxAbsScaler()\n","  elif normalizer == 'PowerTransformer':\n","    norm = PowerTransformer()\n","  elif normalizer == 'QuantileTransformer':\n","    norm = QuantileTransformer(output_distribution='normal')\n","  \n","  df_aux = norm.fit_transform(df_aux)\n","\n","  if pca:\n","    pca = PCA(pca_components, random_state=random_state)\n","    df_aux = pca.fit_transform(df_aux)\n","\n","    return df_aux, enc, norm, pca\n","\n","  else:\n","    return df_aux, enc, norm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpuyLBQmSDsX"},"source":["def test_preprocessing(df, enc, norm, pca=None):\n","  \"\"\"Preprocessing the test dataset.\n","\n","  :param df pd.DataFrame: DataFrame to be preprocessed.\n","  :param enc: trained encoder with the categorical features.\n","  :param norm: trained normalizer.\n","  :param pca: trained PCA (default=None).\n","\n","  :return: preprocessed test DataFrame \n","  :rtype: pd.DataFrame\n","  \"\"\"\n","\n","  df_aux = df.copy()\n","\n","  df_aux.fillna(0, inplace=True)\n","\n","  list_categorical = df_aux.select_dtypes(include='object').columns\n","\n","  for col in list_categorical:\n","    df_aux.loc[~df_aux[col].isin(enc[col].classes_), col] = -1 \n","    df_aux.loc[df_aux[col].isin(enc[col].classes_), col] = enc[col].transform(df_aux[col][df_aux[col].isin(enc[col].classes_)])\n","\n","  df_aux = norm.transform(df_aux)\n","\n","  if pca != None:\n","    df_aux = pca.transform(df_aux)\n","\n","  return df_aux "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jK6Lrb77k7WZ"},"source":["def plot_feat_importances(model, X_test, n=25):\n","  \"\"\"Shows the features importances for the model.\n","\n","  :param model: machine learning model.\n","  :param X_test pd.DataFrame: X_test for the model, before preprocessing.\n","  :param n int: number of features to be shown (default=25).\n","\n","  :return: no value\n","  :rtype: none\n","  \"\"\"\n","\n","  feat_import = pd.Series(model.feature_importances_, index=X_test.columns)\n","  feat_import.nlargest(n).plot(kind='barh', figsize=(10,10))\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mT82Y8uW3FHf"},"source":["def validate_regression(X_test, model, y_test):\n","  \"\"\"Validate the regression.\n","\n","  :param X_test pd.DataFrame: values to be validated \n","  :param model: trained machine learning model\n","  :param y_test array-like: true labels for the regression\n","\n","  :return: DataFrame comparing the real and predicted values\n","  :rtype: pd.DataFrame\n","  \"\"\"\n","\n","  y_pred = model.predict(X_test)\n","  df = pd.DataFrame({'Atual': y_test, 'Predito': y_pred})\n","\n","  print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.3f}')\n","  print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred):.3f}')\n","  print(f'Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}')\n","  print(f'Score: {model.score(X_test, y_test):.3f}')\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pcL2DGLg8DF-"},"source":["# **Referências**"]},{"cell_type":"markdown","metadata":{"id":"rxjnLWTc8GzJ"},"source":["https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n","\n","https://machinelearningmastery.com/robust-regression-for-machine-learning-in-python/\n","\n","https://stackabuse.com/linear-regression-in-python-with-scikit-learn/\n","\n","https://machinelearningmastery.com/xgboost-for-regression/"]}]}