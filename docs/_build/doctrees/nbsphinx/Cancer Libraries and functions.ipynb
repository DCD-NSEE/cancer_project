{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPae7x8J38aH"
   },
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boYaJCOJc7l_"
   },
   "source": [
    "Import of all libraries used in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:23.310799Z",
     "iopub.status.busy": "2021-05-12T12:57:23.309802Z",
     "iopub.status.idle": "2021-05-12T12:57:25.920594Z",
     "shell.execute_reply": "2021-05-12T12:57:25.919561Z"
    },
    "id": "f_Lpm8RjeXYW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pickle # Save models\n",
    "seed = 10 # seed for the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:25.924412Z",
     "iopub.status.busy": "2021-05-12T12:57:25.923376Z",
     "iopub.status.idle": "2021-05-12T12:57:26.579404Z",
     "shell.execute_reply": "2021-05-12T12:57:26.578407Z"
    },
    "id": "1ivafM-qaTLo"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:26.586695Z",
     "iopub.status.busy": "2021-05-12T12:57:26.585698Z",
     "iopub.status.idle": "2021-05-12T12:57:37.272261Z",
     "shell.execute_reply": "2021-05-12T12:57:37.273258Z"
    },
    "id": "k6QXRUiSfi7w"
   },
   "outputs": [],
   "source": [
    "# Classification\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.280709Z",
     "iopub.status.busy": "2021-05-12T12:57:37.280709Z",
     "iopub.status.idle": "2021-05-12T12:57:37.301157Z",
     "shell.execute_reply": "2021-05-12T12:57:37.302147Z"
    },
    "id": "oMEBkOAi3U1I"
   },
   "outputs": [],
   "source": [
    "# Regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otqVd1m7YMs1"
   },
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL3L1f4VeBUW"
   },
   "source": [
    "Here we have some functions used in the project, the ones for general use are for creating new columns in the data and reading and saving datasets. The functions for the models contain the preprocessing, division into training and test data, validation of the regressions and plot of the graph with the features importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImgVOwGnmMB8"
   },
   "source": [
    "## **General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.312846Z",
     "iopub.status.busy": "2021-05-12T12:57:37.311850Z",
     "iopub.status.idle": "2021-05-12T12:57:37.334848Z",
     "shell.execute_reply": "2021-05-12T12:57:37.333805Z"
    },
    "id": "q-lnAaaNYPbJ"
   },
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "  \"\"\"Read csv files\n",
    "\n",
    "  :param path str: path to the csv file.\n",
    "\n",
    "  :return: dataframe from the csv file.\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df = pd.read_csv(path)\n",
    "  print(df.shape)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.345189Z",
     "iopub.status.busy": "2021-05-12T12:57:37.343514Z",
     "iopub.status.idle": "2021-05-12T12:57:37.349238Z",
     "shell.execute_reply": "2021-05-12T12:57:37.349238Z"
    },
    "id": "1_ogS1lWYkAw"
   },
   "outputs": [],
   "source": [
    "def save_csv(df, path):\n",
    "  \"\"\"Save csv files\n",
    "\n",
    "  :param df pd.DataFrame: dataframe to be saved.\n",
    "  :param path str: path to save the csv file.\n",
    "\n",
    "  :return: no value\n",
    "  :rtype: none\n",
    "  \"\"\"\n",
    "\n",
    "  df.to_csv(path, encoding='utf-8', index=False)\n",
    "  print('CSV file saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.366741Z",
     "iopub.status.busy": "2021-05-12T12:57:37.366741Z",
     "iopub.status.idle": "2021-05-12T12:57:37.386934Z",
     "shell.execute_reply": "2021-05-12T12:57:37.386934Z"
    },
    "id": "-ghMwgbSVB-T"
   },
   "outputs": [],
   "source": [
    "def get_dates_diff(df):\n",
    "  \"\"\"Get the difference, in days, between columns with dates\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to get the dates difference.\n",
    "\n",
    "  :return: DataFrame with dates difference in nine new columns \n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "  \n",
    "  df_aux = df.copy()\n",
    "  \n",
    "  df_aux.dropna(subset=['DTTRAT','DTULTINFO'], inplace=True)\n",
    "\n",
    "  lista_datas = ['DTCONSULT', 'DTDIAG', 'DTTRAT', 'DTULTINFO', 'DTRECIDIVA']\n",
    "  \n",
    "  for c in lista_datas:\n",
    "    if c == 'DTTRAT': # Has a different date format \n",
    "      fmt = '%d/%m/%Y'\n",
    "    else:\n",
    "      fmt = '%Y-%m-%d'\n",
    "    df_aux[c] = pd.to_datetime(df_aux[c], format=fmt)\n",
    "\n",
    "  df_aux['delta_t1'] = (df_aux.DTDIAG - df_aux.DTCONSULT).dt.days\n",
    "  df_aux['delta_t2'] = (df_aux.DTTRAT - df_aux.DTDIAG).dt.days\n",
    "  df_aux['delta_t3'] = (df_aux.DTTRAT - df_aux.DTCONSULT).dt.days\n",
    "\n",
    "  df_aux['delta_t4'] = (df_aux.DTRECIDIVA - df_aux.DTCONSULT).dt.days\n",
    "  df_aux['delta_t5'] = (df_aux.DTRECIDIVA - df_aux.DTDIAG).dt.days\n",
    "  df_aux['delta_t6'] = (df_aux.DTRECIDIVA - df_aux.DTTRAT).dt.days\n",
    "\n",
    "  df_aux['delta_t7'] = (df_aux.DTULTINFO - df_aux.DTCONSULT).dt.days\n",
    "  df_aux['delta_t8'] = (df_aux.DTULTINFO - df_aux.DTDIAG).dt.days\n",
    "  df_aux['delta_t9'] = (df_aux.DTULTINFO - df_aux.DTTRAT).dt.days\n",
    "\n",
    "  return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.403327Z",
     "iopub.status.busy": "2021-05-12T12:57:37.402338Z",
     "iopub.status.idle": "2021-05-12T12:57:37.425898Z",
     "shell.execute_reply": "2021-05-12T12:57:37.425898Z"
    },
    "id": "sCzT84kOwx0D"
   },
   "outputs": [],
   "source": [
    "def get_labels(df):\n",
    "  \"\"\"Create death labels acording to the last information year.\n",
    "\n",
    "  :param df pd.DataFrame: dataframe to be processed.\n",
    "\n",
    "  :return: DataFrame with the new labels\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  # Label de óbito\n",
    "  df_aux['ob'] = 0\n",
    "\n",
    "  # Label de óbito de acordo com o ano\n",
    "  df_aux['ano_ob'] = 0\n",
    "  \n",
    "  for index, row in df_aux.iterrows():\n",
    "    if row.ULTINFO > 2:\n",
    "      df_aux.loc[index, 'ob'] = 1\n",
    "      if row.delta_t8 < 365:\n",
    "        df_aux.loc[index, 'ano_ob'] = 1\n",
    "      elif row.delta_t8 < 2*365:\n",
    "        df_aux.loc[index, 'ano_ob'] = 2\n",
    "      elif row.delta_t8 < 3*365:\n",
    "        df_aux.loc[index, 'ano_ob'] = 3\n",
    "      elif row.delta_t8 < 4*365:\n",
    "        df_aux.loc[index, 'ano_ob'] = 4\n",
    "      elif row.delta_t8 < 5*365:\n",
    "        df_aux.loc[index, 'ano_ob'] = 5\n",
    "      else:\n",
    "        df_aux.loc[index, 'ano_ob'] = 6\n",
    "\n",
    "  return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.439424Z",
     "iopub.status.busy": "2021-05-12T12:57:37.439424Z",
     "iopub.status.idle": "2021-05-12T12:57:37.459623Z",
     "shell.execute_reply": "2021-05-12T12:57:37.459623Z"
    },
    "id": "bJrOmtBTUJEF"
   },
   "outputs": [],
   "source": [
    "def get_label_rec(df):\n",
    "  \"\"\"Create the labels analyzing whether there was recurrence.\n",
    "  \n",
    "  :param df pd.DataFrame: dataframe to be processed.\n",
    "\n",
    "  :return: DataFrame with the new labels\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  df_aux['ob_com_rec'] = 0\n",
    "  df_aux['ob_sem_rec'] = 0\n",
    "  df_aux['vivo_com_rec'] = 0\n",
    "  df_aux['vivo_sem_rec'] = 0\n",
    "\n",
    "  for i, row in df.iterrows():\n",
    "    if row['ob'] == 1:\n",
    "      if row.RECNENHUM == 1:\n",
    "        df_aux.loc[i, 'ob_sem_rec'] = 1\n",
    "      else:\n",
    "        df_aux.loc[i, 'ob_com_rec'] = 1\n",
    "        \n",
    "    else:\n",
    "      if row.RECNENHUM == 1:\n",
    "        df_aux.loc[i, 'vivo_sem_rec'] = 1\n",
    "      else:\n",
    "        df_aux.loc[i, 'vivo_com_rec'] = 1\n",
    "\n",
    "  return df_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUQsVex8mQZp"
   },
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.484108Z",
     "iopub.status.busy": "2021-05-12T12:57:37.483055Z",
     "iopub.status.idle": "2021-05-12T12:57:37.505969Z",
     "shell.execute_reply": "2021-05-12T12:57:37.504827Z"
    },
    "id": "LdQRlaLJ3sum"
   },
   "outputs": [],
   "source": [
    "def variables_preprocessing(df):\n",
    "  \"\"\"Do some preprocessing on the DataFrame like strings splits, fill NaN values,\n",
    "     replace values and drop some columns.\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "\n",
    "  :return: DataFrame after be preprocessed and get some columns removed\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "  no_info = '**Sem informação**'\n",
    "\n",
    "  # DRS\n",
    "  DRS_expand = df_aux.DRS.str.split(' ', expand=True)\n",
    "  df_aux['DRS'] = DRS_expand[1]\n",
    "  df_aux.DRS = df_aux.DRS.fillna(0).astype('int64')\n",
    "\n",
    "  # META\n",
    "  df_aux.META01.fillna(no_info, inplace=True)\n",
    "  df_aux.META02.fillna(no_info, inplace=True)\n",
    "  df_aux.META03.fillna(no_info, inplace=True)\n",
    "  df_aux.META04.fillna(no_info, inplace=True)\n",
    "\n",
    "  # REC\n",
    "  df_aux.REC01.fillna(no_info, inplace=True)\n",
    "  df_aux.REC02.fillna(no_info, inplace=True)\n",
    "  df_aux.REC03.fillna(no_info, inplace=True)\n",
    "  df_aux.REC04.fillna(no_info, inplace=True)\n",
    "\n",
    "  # PT\n",
    "  df_aux.PT = df_aux.PT.replace([1.0],'1')\n",
    "  df_aux.PT = df_aux.PT.str.upper()\n",
    "  df_aux.PT.fillna(no_info, inplace=True)\n",
    "\n",
    "  # PN\n",
    "  df_aux.PN = df_aux.PN.replace([0.0],'0')\n",
    "  df_aux.PN = df_aux.PN.str.upper()\n",
    "  df_aux.PN = df_aux.PN.replace(['1BI','IBII','O'],['1B1','1B2','0'])\n",
    "  df_aux.PN.fillna(no_info, inplace=True)\n",
    "\n",
    "  # PM\n",
    "  df_aux.PM = df_aux.PM.replace([0.0],'0')\n",
    "  df_aux.PM.fillna(no_info, inplace=True)\n",
    "\n",
    "  # CICI\n",
    "  df_aux.CICI.fillna(no_info, inplace=True)\n",
    "\n",
    "  # CICIGRUP\n",
    "  CICIGRUP_expand = df_aux.CICIGRUP.str.split('  ', expand=True)\n",
    "  df_aux['CICIGRUP'] = CICIGRUP_expand[0]\n",
    "  df_aux.CICIGRUP.fillna(no_info, inplace=True)\n",
    "\n",
    "  # Colunas com valores únicos \n",
    "  col = df_aux.columns\n",
    "  drop_cols = ['S','QUIMIOANT','HORMOANT','TMOANT','IMUNOANT','OUTROANT','ERRO',\n",
    "               'CIDO', 'UFNASC','CIDADE','DESCTOPO','DESCMORFO','DSCCIDO','CICISUBGRU',\n",
    "               'INSTORIG', 'OUTRACLA']\n",
    "\n",
    "  col = col.drop(drop_cols)\n",
    "\n",
    "  return df_aux[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.518154Z",
     "iopub.status.busy": "2021-05-12T12:57:37.517108Z",
     "iopub.status.idle": "2021-05-12T12:57:37.538292Z",
     "shell.execute_reply": "2021-05-12T12:57:37.539288Z"
    },
    "id": "r69BRdrcISkx"
   },
   "outputs": [],
   "source": [
    "def get_train_test(df, drop_cols, label, test_size=0.25, random_state=10):\n",
    "  \"\"\"Get features and label, and then returns train and test dataframes.\n",
    "\n",
    "  :param df pd.DataFrame: dataframe that will be splitted.\n",
    "  :param drop_cols list: columns to be removed from the DataFrame.\n",
    "  :param label str: name of the label column.\n",
    "  :param test_size float: size of test (default=0.25).\n",
    "  :param random_state int: value for train_test_split random_state (default=10).\n",
    "\n",
    "  :return: train and test DataFrames, X_train, X_test, y_train, y_test\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  cols = df_aux.columns.drop(drop_cols)\n",
    "  lb = df_aux[label].copy()\n",
    "  cols = cols.drop(label)\n",
    "  feat = df_aux[cols]\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(feat, lb, test_size=test_size, random_state=random_state)\n",
    "  print(f'X_train = {X_train.shape}, X_test = {X_test.shape}')\n",
    "  print(f'y_train = {y_train.shape}, y_test = {y_test.shape}')\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.554884Z",
     "iopub.status.busy": "2021-05-12T12:57:37.554884Z",
     "iopub.status.idle": "2021-05-12T12:57:37.576198Z",
     "shell.execute_reply": "2021-05-12T12:57:37.575202Z"
    },
    "id": "XFAnM9raPU_T"
   },
   "outputs": [],
   "source": [
    "def train_preprocessing(df, normalizer='StandardScaler', pca=False, pca_components=None, random_state=10):\n",
    "  \"\"\"Preprocessing the train dataset.\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "  :param normalizer str: which normalizer to be fitted to the data (default='StandardScaler').\n",
    "  :param pca bool: if want to use PCA components set True (default=False).\n",
    "  :param pca_components int: number of PCA components (default=None).\n",
    "  :param random_state int: value for pca random_state (default=10).\n",
    "\n",
    "  :return df: preprocessed train DataFrame \n",
    "  :rtype: pd.DataFrame\n",
    "  :return enc: trained LabelEncoder \n",
    "  :rtype: dict\n",
    "  :return norm: trained normalizer \n",
    "  :rtype: object\n",
    "  :return pca if param pca=True: trained PCA \n",
    "  :rtype: object\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  list_categorical = df_aux.select_dtypes(include='object').columns\n",
    "\n",
    "  enc = dict()\n",
    "  for col in list_categorical:\n",
    "    enc[col] = LabelEncoder()\n",
    "    df_aux[col] = enc[col].fit_transform(df_aux[col])\n",
    "\n",
    "  if normalizer == 'StandardScaler':\n",
    "    norm = StandardScaler()\n",
    "  elif normalizer == 'MinMaxScaler':\n",
    "    norm = MinMaxScaler()\n",
    "  elif normalizer == 'MaxAbsScaler':\n",
    "    norm = MaxAbsScaler()\n",
    "  elif normalizer == 'PowerTransformer':\n",
    "    norm = PowerTransformer()\n",
    "  elif normalizer == 'QuantileTransformer':\n",
    "    norm = QuantileTransformer(output_distribution='normal')\n",
    "  \n",
    "  df_aux = norm.fit_transform(df_aux)\n",
    "\n",
    "  if pca:\n",
    "    pca = PCA(pca_components, random_state=random_state)\n",
    "    df_aux = pca.fit_transform(df_aux)\n",
    "\n",
    "    return df_aux, enc, norm, pca\n",
    "\n",
    "  else:\n",
    "    return df_aux, enc, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.589259Z",
     "iopub.status.busy": "2021-05-12T12:57:37.589259Z",
     "iopub.status.idle": "2021-05-12T12:57:37.610815Z",
     "shell.execute_reply": "2021-05-12T12:57:37.611808Z"
    },
    "id": "FpuyLBQmSDsX"
   },
   "outputs": [],
   "source": [
    "def test_preprocessing(df, enc, norm, pca=None):\n",
    "  \"\"\"Preprocessing the test dataset.\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "  :param enc: trained encoder with the categorical features.\n",
    "  :param norm: trained normalizer.\n",
    "  :param pca: trained PCA (default=None).\n",
    "\n",
    "  :return: preprocessed test DataFrame \n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  df_aux.fillna(0, inplace=True)\n",
    "\n",
    "  list_categorical = df_aux.select_dtypes(include='object').columns\n",
    "\n",
    "  for col in list_categorical:\n",
    "    df_aux.loc[~df_aux[col].isin(enc[col].classes_), col] = -1 \n",
    "    df_aux.loc[df_aux[col].isin(enc[col].classes_), col] = enc[col].transform(df_aux[col][df_aux[col].isin(enc[col].classes_)])\n",
    "\n",
    "  df_aux = norm.transform(df_aux)\n",
    "\n",
    "  if pca != None:\n",
    "    df_aux = pca.transform(df_aux)\n",
    "\n",
    "  return df_aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.621324Z",
     "iopub.status.busy": "2021-05-12T12:57:37.621324Z",
     "iopub.status.idle": "2021-05-12T12:57:37.641810Z",
     "shell.execute_reply": "2021-05-12T12:57:37.641810Z"
    },
    "id": "jK6Lrb77k7WZ"
   },
   "outputs": [],
   "source": [
    "def plot_feat_importances(model, X_test, n=25):\n",
    "  \"\"\"Shows the features importances for the model.\n",
    "\n",
    "  :param model: machine learning model.\n",
    "  :param X_test pd.DataFrame: X_test for the model, before preprocessing.\n",
    "  :param n int: number of features to be shown (default=25).\n",
    "\n",
    "  :return: no value\n",
    "  :rtype: none\n",
    "  \"\"\"\n",
    "\n",
    "  feat_import = pd.Series(model.feature_importances_, index=X_test.columns)\n",
    "  feat_import.nlargest(n).plot(kind='barh', figsize=(10,10))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-12T12:57:37.654050Z",
     "iopub.status.busy": "2021-05-12T12:57:37.653053Z",
     "iopub.status.idle": "2021-05-12T12:57:37.674585Z",
     "shell.execute_reply": "2021-05-12T12:57:37.674585Z"
    },
    "id": "mT82Y8uW3FHf"
   },
   "outputs": [],
   "source": [
    "def validate_regression(X_test, model, y_test):\n",
    "  \"\"\"Validate the regression.\n",
    "\n",
    "  :param X_test pd.DataFrame: values to be validated \n",
    "  :param model: trained machine learning model\n",
    "  :param y_test array-like: true labels for the regression\n",
    "\n",
    "  :return: DataFrame comparing the real and predicted values\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  y_pred = model.predict(X_test)\n",
    "  df = pd.DataFrame({'Atual': y_test, 'Predito': y_pred})\n",
    "\n",
    "  print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.3f}')\n",
    "  print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "  print(f'Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}')\n",
    "  print(f'Score: {model.score(X_test, y_test):.3f}')\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcL2DGLg8DF-"
   },
   "source": [
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxjnLWTc8GzJ"
   },
   "source": [
    "[Pandas](https://pandas.pydata.org/docs/reference/index.html)\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/api.html)\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/stable/gallery/index.html)\n",
    "\n",
    "[Plotly](https://plotly.com/python/)\n",
    "\n",
    "[sklearn preprocessing](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)\n",
    "\n",
    "[sklearn train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)\n",
    "\n",
    "[imblearn](https://imbalanced-learn.org/stable/references/index.html)\n",
    "\n",
    "[Random Forest Classifier and Regressor](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)\n",
    "\n",
    "[Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix)\n",
    "\n",
    "[Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "\n",
    "[XGBoost for Regression Machine Learning Mastery](https://machinelearningmastery.com/xgboost-for-regression/)\n",
    "\n",
    "https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "\n",
    "https://machinelearningmastery.com/robust-regression-for-machine-learning-in-python/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+Nh7SwmFx2dP2kRcEuitc",
   "collapsed_sections": [
    "LPae7x8J38aH",
    "otqVd1m7YMs1",
    "ImgVOwGnmMB8",
    "tUQsVex8mQZp",
    "pcL2DGLg8DF-"
   ],
   "mount_file_id": "1fR9g2luc55coRAcv8aFtP6GrADp0y0XW",
   "name": "Cancer Libraries and functions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
