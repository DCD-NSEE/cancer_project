{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPae7x8J38aH"
   },
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boYaJCOJc7l_"
   },
   "source": [
    "Import of all libraries used in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:17.962261Z",
     "iopub.status.busy": "2021-06-04T19:50:17.961261Z",
     "iopub.status.idle": "2021-06-04T19:50:22.223349Z",
     "shell.execute_reply": "2021-06-04T19:50:22.222351Z"
    },
    "id": "f_Lpm8RjeXYW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pickle # Save models\n",
    "seed = 10 # seed for the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:22.234356Z",
     "iopub.status.busy": "2021-06-04T19:50:22.232631Z",
     "iopub.status.idle": "2021-06-04T19:50:23.354490Z",
     "shell.execute_reply": "2021-06-04T19:50:23.355489Z"
    },
    "id": "1ivafM-qaTLo"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:23.363776Z",
     "iopub.status.busy": "2021-06-04T19:50:23.362187Z",
     "iopub.status.idle": "2021-06-04T19:50:23.984802Z",
     "shell.execute_reply": "2021-06-04T19:50:23.985793Z"
    },
    "id": "k6QXRUiSfi7w"
   },
   "outputs": [],
   "source": [
    "# Classification\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otqVd1m7YMs1"
   },
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL3L1f4VeBUW"
   },
   "source": [
    "Here we have some functions used in the project, the ones for general use are for creating new columns in the data and reading and saving datasets. The functions for the models contain the preprocessing, division into training and test data, validation of the regressions and plot of the graph with the features importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImgVOwGnmMB8"
   },
   "source": [
    "## **General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:23.998514Z",
     "iopub.status.busy": "2021-06-04T19:50:23.996275Z",
     "iopub.status.idle": "2021-06-04T19:50:24.015457Z",
     "shell.execute_reply": "2021-06-04T19:50:24.017370Z"
    },
    "id": "q-lnAaaNYPbJ"
   },
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "  \"\"\"\"Read csv files\n",
    "\n",
    "  :param path str: path to the csv file.\n",
    "\n",
    "  :return: dataframe from the csv file.\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df = pd.read_csv(path, dtype={'M': str})\n",
    "  print(df.shape)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.033424Z",
     "iopub.status.busy": "2021-06-04T19:50:24.030425Z",
     "iopub.status.idle": "2021-06-04T19:50:24.048163Z",
     "shell.execute_reply": "2021-06-04T19:50:24.048921Z"
    },
    "id": "1_ogS1lWYkAw"
   },
   "outputs": [],
   "source": [
    "def save_csv(df, path):\n",
    "  \"\"\"Save csv files\n",
    "\n",
    "  :param df pd.DataFrame: dataframe to be saved.\n",
    "  :param path str: path to save the csv file.\n",
    "\n",
    "  :return: no value\n",
    "  :rtype: none\n",
    "  \"\"\"\n",
    "\n",
    "  df.to_csv(path, encoding='utf-8', index=False)\n",
    "  print('CSV file saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.065320Z",
     "iopub.status.busy": "2021-06-04T19:50:24.064315Z",
     "iopub.status.idle": "2021-06-04T19:50:24.079527Z",
     "shell.execute_reply": "2021-06-04T19:50:24.080527Z"
    },
    "id": "-ghMwgbSVB-T"
   },
   "outputs": [],
   "source": [
    "def get_dates_diff(df, dates_list):\n",
    "  \"\"\"Get the difference, in days, between columns with dates\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to get the dates difference.\n",
    "  :param dates_list list: list with the name of date columns.\n",
    "\n",
    "  :return: DataFrame with dates difference in nine new columns \n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "  \n",
    "  df_aux = df.copy()\n",
    "  \n",
    "  df_aux.dropna(subset=['DTTRAT','DTULTINFO'], inplace=True)\n",
    "  \n",
    "  for c in dates_list:\n",
    "    if c in ['DTTRAT','DTULTINFO','DTRECIDIVA']: # Has a different date format \n",
    "      fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    else:\n",
    "      fmt = '%Y-%m-%d'\n",
    "    df_aux[c] = pd.to_datetime(df_aux[c], format=fmt)\n",
    "\n",
    "  df_aux['delta_t1'] = (df_aux.DTDIAG - df_aux.DTCONSULT).dt.days\n",
    "  df_aux['delta_t2'] = (df_aux.DTTRAT - df_aux.DTDIAG).dt.days\n",
    "  df_aux['delta_t3'] = (df_aux.DTTRAT - df_aux.DTCONSULT).dt.days\n",
    "\n",
    "  df_aux['delta_t4'] = (df_aux.DTRECIDIVA - df_aux.DTCONSULT).dt.days\n",
    "  df_aux['delta_t5'] = (df_aux.DTRECIDIVA - df_aux.DTDIAG).dt.days\n",
    "  df_aux['delta_t6'] = (df_aux.DTRECIDIVA - df_aux.DTTRAT).dt.days\n",
    "\n",
    "  df_aux['delta_t7'] = (df_aux.DTULTINFO - df_aux.DTCONSULT).dt.days\n",
    "  df_aux['delta_t8'] = (df_aux.DTULTINFO - df_aux.DTDIAG).dt.days\n",
    "  df_aux['delta_t9'] = (df_aux.DTULTINFO - df_aux.DTTRAT).dt.days\n",
    "\n",
    "  return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.092041Z",
     "iopub.status.busy": "2021-06-04T19:50:24.090037Z",
     "iopub.status.idle": "2021-06-04T19:50:24.110136Z",
     "shell.execute_reply": "2021-06-04T19:50:24.111130Z"
    },
    "id": "ow3gHyqs3_YG"
   },
   "outputs": [],
   "source": [
    "def get_labels(df):\n",
    "  \"\"\"Create death labels acording to the last information year.\n",
    "\n",
    "  :param df pd.DataFrame: dataframe to be processed.\n",
    "\n",
    "  :return: DataFrame with the new labels\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  df_aux['ob'] = 0\n",
    "\n",
    "  df_aux['vivo_ano1'] = 0\n",
    "  df_aux['vivo_ano3'] = 0\n",
    "  df_aux['vivo_ano5'] = 0 \n",
    "  \n",
    "  df_aux.loc[df_aux.ULTINFO > 2, 'ob'] = 1\n",
    "\n",
    "  df_aux.loc[df_aux.delta_t8 > 365, 'vivo_ano1'] = 1\n",
    "  df_aux.loc[df_aux.delta_t8 > 3*365, 'vivo_ano3'] = 1\n",
    "  df_aux.loc[df_aux.delta_t8 > 5*365, 'vivo_ano5'] = 1\n",
    "\n",
    "  return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.120132Z",
     "iopub.status.busy": "2021-06-04T19:50:24.120132Z",
     "iopub.status.idle": "2021-06-04T19:50:24.141563Z",
     "shell.execute_reply": "2021-06-04T19:50:24.142542Z"
    },
    "id": "bJrOmtBTUJEF"
   },
   "outputs": [],
   "source": [
    "def get_label_rec(df):\n",
    "  \"\"\"Create the labels analyzing whether there was recurrence.\n",
    "  \n",
    "  :param df pd.DataFrame: dataframe to be processed.\n",
    "\n",
    "  :return: DataFrame with the new labels\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  df_aux['ob_com_rec'] = 0\n",
    "  df_aux['ob_sem_rec'] = 0\n",
    "  df_aux['vivo_com_rec'] = 0\n",
    "  df_aux['vivo_sem_rec'] = 0\n",
    "\n",
    "  df_aux.loc[(df_aux.ob == 1) & (df_aux.RECNENHUM == 1), 'ob_sem_rec'] = 1\n",
    "  df_aux.loc[(df_aux.ob == 1) & (df_aux.RECNENHUM == 0), 'ob_com_rec'] = 1\n",
    "  df_aux.loc[(df_aux.ob == 0) & (df_aux.RECNENHUM == 1), 'vivo_sem_rec'] = 1\n",
    "  df_aux.loc[(df_aux.ob == 0) & (df_aux.RECNENHUM == 0), 'vivo_com_rec'] = 1\n",
    "\n",
    "  return df_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUQsVex8mQZp"
   },
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.159549Z",
     "iopub.status.busy": "2021-06-04T19:50:24.158549Z",
     "iopub.status.idle": "2021-06-04T19:50:24.171864Z",
     "shell.execute_reply": "2021-06-04T19:50:24.174488Z"
    },
    "id": "LdQRlaLJ3sum"
   },
   "outputs": [],
   "source": [
    "def variables_preprocessing(df):\n",
    "  \"\"\"Do some preprocessing on the DataFrame like strings splits, fill NaN values,\n",
    "     replace values and drop some columns.\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "\n",
    "  :return: DataFrame after be preprocessed and get some columns removed\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "  no_info = '**Sem informação**'\n",
    "\n",
    "  # Excluding ECGRUP with X and Y values\n",
    "  df_aux = df_aux[~df_aux.ECGRUP.isin(['X','Y'])]\n",
    "\n",
    "  # Excluding C44 topographies (with morphologies between 80101 and 81103)\n",
    "  df_aux = df_aux[~((df_aux.TOPOGRUP == 'C44') & (df_aux.MORFO >= 80101) & (df_aux.MORFO <= 81103))]\n",
    "\n",
    "  # REGISTRADO\n",
    "  df_aux.REGISTRADO.fillna(99, inplace=True)\n",
    "\n",
    "  # DRS\n",
    "  DRS_expand = df_aux.DRS.str.split(' ', expand=True)\n",
    "  df_aux['DRS'] = DRS_expand[1]\n",
    "  df_aux.DRS.fillna(0, inplace=True)\n",
    "\n",
    "  # META\n",
    "  df_aux.META01.fillna(no_info, inplace=True)\n",
    "  df_aux.META02.fillna(no_info, inplace=True)\n",
    "\n",
    "  # REC\n",
    "  df_aux.REC01.fillna(no_info, inplace=True)\n",
    "  df_aux.REC02.fillna(no_info, inplace=True)\n",
    "  df_aux.REC03.fillna(no_info, inplace=True)\n",
    "\n",
    "  # PT\n",
    "  df_aux.PT = df_aux.PT.str.upper()\n",
    "  df_aux.PT.fillna(no_info, inplace=True)\n",
    "\n",
    "  # PN\n",
    "  df_aux.PN.fillna(no_info, inplace=True)\n",
    "\n",
    "  # PM\n",
    "  df_aux.PM.fillna(no_info, inplace=True)\n",
    "\n",
    "  col = df_aux.columns\n",
    "  drop_cols = ['S','QUIMIOANT','HORMOANT','TMOANT','IMUNOANT','OUTROANT','UFNASC',\n",
    "               'CICISUBGRU','CICIGRUP','CICI','META04','META03','REC04','CIDADE',\n",
    "               'DESCTOPO','DESCMORFO','INSTORIG','OUTRACLA']\n",
    "\n",
    "  col = col.drop(drop_cols)\n",
    "\n",
    "  return df_aux[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.183447Z",
     "iopub.status.busy": "2021-06-04T19:50:24.178447Z",
     "iopub.status.idle": "2021-06-04T19:50:24.187450Z",
     "shell.execute_reply": "2021-06-04T19:50:24.188916Z"
    },
    "id": "r69BRdrcISkx"
   },
   "outputs": [],
   "source": [
    "def get_train_test(df, drop_cols, label, test_size=0.25, random_state=0):\n",
    "  \"\"\"Get features and label, and then returns train and test dataframes.\n",
    "\n",
    "  :param df pd.DataFrame: dataframe that will be splitted.\n",
    "  :param drop_cols list: columns to be removed from the DataFrame.\n",
    "  :param label str: name of the label column.\n",
    "  :param test_size float: size of test (default=0.25).\n",
    "  :param random_state int: value for train_test_split random_state (default=10).\n",
    "\n",
    "  :return: train and test DataFrames, X_train, X_test, y_train, y_test\n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  cols = df_aux.columns.drop(drop_cols)\n",
    "  lb = df_aux[label].copy()\n",
    "  cols = cols.drop(label)\n",
    "  feat = df_aux[cols]\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(feat, lb, test_size=test_size, random_state=random_state)\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.201972Z",
     "iopub.status.busy": "2021-06-04T19:50:24.200976Z",
     "iopub.status.idle": "2021-06-04T19:50:24.219221Z",
     "shell.execute_reply": "2021-06-04T19:50:24.218224Z"
    },
    "id": "XFAnM9raPU_T"
   },
   "outputs": [],
   "source": [
    "def train_preprocessing(df, normalizer='StandardScaler', pca=False, \n",
    "                        pca_components=None, random_state=0):\n",
    "  \"\"\"Preprocessing the train dataset.\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "  :param normalizer str: which normalizer to be fitted to the data (default='StandardScaler').\n",
    "    options:\n",
    "      * 'MinMaxScaler'\n",
    "      * 'MaxAbsScaler'\n",
    "      * 'PowerTransformer'\n",
    "      * 'QuantileTransformer'\n",
    "  :param pca bool: if want to use PCA components set True (default=False).\n",
    "  :param pca_components int: number of PCA components (default=None).\n",
    "  :param random_state int: value for pca random_state (default=10).\n",
    "\n",
    "  :return df: preprocessed train DataFrame \n",
    "  :rtype: pd.DataFrame\n",
    "  :return enc: trained LabelEncoder \n",
    "  :rtype: dict\n",
    "  :return norm: trained normalizer \n",
    "  :rtype: object\n",
    "  :return pca if param pca=True: trained PCA \n",
    "  :rtype: object\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  list_categorical = df_aux.select_dtypes(include='object').columns\n",
    "\n",
    "  enc = dict()\n",
    "  for col in list_categorical:\n",
    "    enc[col] = LabelEncoder()\n",
    "    df_aux[col] = enc[col].fit_transform(df_aux[col])\n",
    "\n",
    "  if normalizer == 'StandardScaler':\n",
    "    norm = StandardScaler()\n",
    "  elif normalizer == 'MinMaxScaler':\n",
    "    norm = MinMaxScaler()\n",
    "  elif normalizer == 'MaxAbsScaler':\n",
    "    norm = MaxAbsScaler()\n",
    "  elif normalizer == 'PowerTransformer':\n",
    "    norm = PowerTransformer()\n",
    "  elif normalizer == 'QuantileTransformer':\n",
    "    norm = QuantileTransformer(output_distribution='normal')\n",
    "  \n",
    "  df_aux = norm.fit_transform(df_aux)\n",
    "\n",
    "  if pca:\n",
    "    pca = PCA(pca_components, random_state=random_state)\n",
    "    df_aux = pca.fit_transform(df_aux)\n",
    "\n",
    "    return df_aux, enc, norm, pca\n",
    "\n",
    "  else:\n",
    "    return df_aux, enc, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.229137Z",
     "iopub.status.busy": "2021-06-04T19:50:24.229137Z",
     "iopub.status.idle": "2021-06-04T19:50:24.234141Z",
     "shell.execute_reply": "2021-06-04T19:50:24.235139Z"
    },
    "id": "FpuyLBQmSDsX"
   },
   "outputs": [],
   "source": [
    "def test_preprocessing(df, enc, norm, pca=None):\n",
    "  \"\"\"Preprocessing the test dataset.\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "  :param enc: trained encoder with the categorical features.\n",
    "  :param norm: trained normalizer.\n",
    "  :param pca: trained PCA (default=None).\n",
    "\n",
    "  :return: preprocessed test DataFrame \n",
    "  :rtype: pd.DataFrame\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  df_aux.fillna(0, inplace=True)\n",
    "\n",
    "  list_categorical = df_aux.select_dtypes(include='object').columns\n",
    "\n",
    "  for col in list_categorical:\n",
    "    df_aux.loc[~df_aux[col].isin(enc[col].classes_), col] = -1 \n",
    "    df_aux.loc[df_aux[col].isin(enc[col].classes_), col] = enc[col].transform(df_aux[col][df_aux[col].isin(enc[col].classes_)])\n",
    "\n",
    "  df_aux = norm.transform(df_aux)\n",
    "\n",
    "  if pca != None:\n",
    "    df_aux = pca.transform(df_aux)\n",
    "\n",
    "  return df_aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.249449Z",
     "iopub.status.busy": "2021-06-04T19:50:24.239211Z",
     "iopub.status.idle": "2021-06-04T19:50:24.265452Z",
     "shell.execute_reply": "2021-06-04T19:50:24.266451Z"
    },
    "id": "wuTX-ljddmAr"
   },
   "outputs": [],
   "source": [
    "def preprocessing(df, cols_drop, label, test_size=0.25, norm_name='StandardScaler', \n",
    "                  pca=False, pca_components=None, balance_data=True, group_years=False,\n",
    "                  first_year=None, last_year=None, morpho3=False, random_state=0):\n",
    "  \n",
    "  \"\"\"Preprocessing the train and test datasets.\n",
    "\n",
    "  :param df pd.DataFrame: DataFrame to be preprocessed.\n",
    "  :param cols_drop list: list of columns to be dropped from dataset.\n",
    "  :param label string: name of the column that will be the label.\n",
    "  :param test_size float: size of test set (default=0.25).\n",
    "  :param normalizer str: which normalizer to be fitted to the data (default='StandardScaler').\n",
    "    - options:\n",
    "      * 'StandardScaler';\n",
    "      * 'MinMaxScaler';\n",
    "      * 'MaxAbsScaler';\n",
    "      * 'PowerTransformer';\n",
    "      * 'QuantileTransformer'.\n",
    "  :param pca bool: if want to use PCA components set True (default=False).\n",
    "  :param pca_components int: number of PCA components (default=None).\n",
    "  :param balance_data bool: balance the data using oversampling (default=True).\n",
    "  :param group_years bool: create a subset with years grouped (default=False).\n",
    "  :param first_year int: first year of the grouped years. Ignored if group_years = False.\n",
    "  :param last_year int: last year of the grouped years. Ignored if group_years = False.\n",
    "  :param morpho3 bool: use only morphologies that the last number is equal to 3 (default=False).\n",
    "  :param random_state int: value for pca random_state (default=10).\n",
    "\n",
    "  :return X_train_: preprocessed train DataFrame \n",
    "  :rtype: pd.DataFrame\n",
    "  :return X_test_: preprocessed test DataFrame \n",
    "  :rtype: pd.DataFrame\n",
    "  :return y_train_: preprocessed train label \n",
    "  :rtype: pd.DataFrame\n",
    "  :return y_test: preprocessed test label \n",
    "  :rtype: pd.DataFrame\n",
    "  :return feat_cols: list with the features columns names\n",
    "  :rtype: list\n",
    "  \"\"\"\n",
    "\n",
    "  df_aux = df.copy()\n",
    "\n",
    "  # Morphology 3\n",
    "  if morpho3:\n",
    "    df_aux['comportamento'] = [int(repr(i)[-1]) for i in df_aux.MORFO]\n",
    "    df_aux = df_aux[df_aux.comportamento == 3].copy()\n",
    "    df_aux.drop(columns='comportamento', inplace=True)\n",
    "\n",
    "  # Grouped years\n",
    "  if group_years and first_year != None and last_year != None:\n",
    "    df_aux = df_aux[(df_aux.ANODIAG >= first_year) & (df_aux.ANODIAG <= last_year)].copy()\n",
    "    \n",
    "  # Train Test split\n",
    "  X_train, X_test, y_train, y_test = get_train_test(df_aux, cols_drop, label, test_size, \n",
    "                                                    random_state=random_state)\n",
    "  feat_cols = X_train.columns\n",
    "\n",
    "  # Preprocessing\n",
    "  if pca and pca_components != None:\n",
    "    X_train_enc, enc, norm, pca = train_preprocessing(X_train, normalizer=norm_name, \n",
    "                                                      pca=pca, pca_components=pca_components,\n",
    "                                                      random_state=random_state)\n",
    "    X_test_ = test_preprocessing(X_test, enc, norm, pca)\n",
    "\n",
    "  else:\n",
    "    X_train_enc, enc, norm = train_preprocessing(X_train, normalizer=norm_name)\n",
    "    X_test_ = test_preprocessing(X_test, enc, norm)\n",
    "\n",
    "  # Balancing\n",
    "  if balance_data:\n",
    "    X_train_, y_train_ = SMOTE(random_state=random_state).fit_resample(X_train_enc, y_train)\n",
    "  \n",
    "  else:\n",
    "    X_train_, y_train_ = X_train_enc, y_train\n",
    "\n",
    "  print(f'X_train = {X_train_.shape}, X_test = {X_test_.shape}')\n",
    "  print(f'y_train = {y_train_.shape}, y_test = {y_test.shape}')\n",
    "\n",
    "  return X_train_, X_test_, y_train_, y_test, feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T19:50:24.273850Z",
     "iopub.status.busy": "2021-06-04T19:50:24.269798Z",
     "iopub.status.idle": "2021-06-04T19:50:24.281846Z",
     "shell.execute_reply": "2021-06-04T19:50:24.280846Z"
    },
    "id": "jK6Lrb77k7WZ"
   },
   "outputs": [],
   "source": [
    "def plot_feat_importances(model, feat_cols, n=25):\n",
    "  \"\"\"Shows the features importances for the model.\n",
    "\n",
    "  :param model: machine learning model.\n",
    "  :param feat_cols list: list of the features used in the model training.\n",
    "  :param n int: number of features to be shown (default=25).\n",
    "\n",
    "  :return: no value\n",
    "  :rtype: none\n",
    "  \"\"\"\n",
    "\n",
    "  feat_import = pd.Series(model.feature_importances_, index=feat_cols)\n",
    "  feat_import.nlargest(n).plot(kind='barh', figsize=(10,10))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcL2DGLg8DF-"
   },
   "source": [
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxjnLWTc8GzJ"
   },
   "source": [
    "[Pandas](https://pandas.pydata.org/docs/reference/index.html)\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/api.html)\n",
    "\n",
    "[Matplotlib](https://matplotlib.org/stable/gallery/index.html)\n",
    "\n",
    "[Plotly](https://plotly.com/python/)\n",
    "\n",
    "[sklearn preprocessing](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)\n",
    "\n",
    "[sklearn train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)\n",
    "\n",
    "[imblearn](https://imbalanced-learn.org/stable/references/index.html)\n",
    "\n",
    "[Random Forest Classifier and Regressor](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)\n",
    "\n",
    "[Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix)\n",
    "\n",
    "[Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "\n",
    "[XGBoost for Regression Machine Learning Mastery](https://machinelearningmastery.com/xgboost-for-regression/)\n",
    "\n",
    "https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "\n",
    "https://machinelearningmastery.com/robust-regression-for-machine-learning-in-python/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0RX93nPK7HgLiU1MMulzI",
   "collapsed_sections": [
    "LPae7x8J38aH",
    "otqVd1m7YMs1",
    "ImgVOwGnmMB8",
    "tUQsVex8mQZp",
    "pcL2DGLg8DF-"
   ],
   "mount_file_id": "1fR9g2luc55coRAcv8aFtP6GrADp0y0XW",
   "name": "Cancer Libraries and functions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
